{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM93k6Rxingj6XRd2l5PqQp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoFades/COS30049_SmartPlant_Sarawak/blob/main/plantclassifier/PlantRecog_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  Required Libraries\n",
        "\n",
        "Import all required libraries. We're using EfficientNetB0 for lightweight training pipeline as well as classifier for less latency while maintaining higher accuracy, however backbone can be upscaled to B1, B2 and so forth if needed.\n"
      ],
      "metadata": {
        "id": "Te6t6kxwTNfG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MDUhMAvWQN2_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras import layers, models\n",
        "import os\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "# Only allow these extensions\n",
        "VALID_EXTS = (\".jpg\", \".jpeg\", \".png\")\n",
        "\n",
        "def list_valid_files(data_dir):\n",
        "    file_paths = []\n",
        "    class_names = sorted(next(os.walk(data_dir))[1])  # subfolders = class names\n",
        "    class_to_index = {name: idx for idx, name in enumerate(class_names)}\n",
        "\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(data_dir, class_name)\n",
        "        for fname in os.listdir(class_dir):\n",
        "            if fname.lower().endswith(VALID_EXTS):\n",
        "                file_paths.append((os.path.join(class_dir, fname), class_to_index[class_name]))\n",
        "            else:\n",
        "                print(\"Skipping non-image:\", fname)\n",
        "    return file_paths, class_names\n",
        "\n",
        "def decode_img(path, label):\n",
        "    try:\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "        img = tf.image.resize(img, IMG_SIZE) / 255.0  # normalize to [0,1]\n",
        "        return img, tf.one_hot(label, depth=num_classes)\n",
        "    except:\n",
        "        tf.print(\"Corrupted:\", path)\n",
        "        return tf.zeros((224,224,3)), tf.one_hot(label, depth=num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Drive Connection\n",
        "\n",
        "Connect to Google Drive for dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zNYFPh-RVgvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z60GykjdVgVD",
        "outputId": "23fb3d7c-7a96-4e4a-8019-888991d1a409"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Data Preparation\n",
        "\n",
        "Next, data preparation and cleaning. Load datasets and normalise datasets along with any other techniques deemed appropriate.\n"
      ],
      "metadata": {
        "id": "KrKHVkHETqga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/drive/MyDrive/COS30049 - PlantRecog/dataset/Training\", image_size=(224, 224), batch_size=32, label_mode=\"categorical\"\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/drive/MyDrive/COS30049 - PlantRecog/dataset/Validation\", image_size=(224, 224), batch_size=32, label_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "# get number of classes correctly\n",
        "num_classes = len(train_ds.class_names)  # <-- correct way\n",
        "\n",
        "print(num_classes)\n",
        "\n",
        "# Normalization layer\n",
        "normalization_layer = layers.Rescaling(1./255)\n",
        "\n",
        "# Augmentation pipeline (can be tweaked if model underfits due to low data volume)\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),       # random horizontal flip\n",
        "    layers.RandomRotation(0.2),            # rotate up to Â±20%\n",
        "    layers.RandomZoom(0.2),                # zoom in/out\n",
        "    layers.RandomContrast(0.2),            # change contrast\n",
        "])\n",
        "\n",
        "# Apply augmentation only on training data\n",
        "train_ds = train_ds.map(lambda x, y: (data_augmentation(normalization_layer(x)), y))\n",
        "val_ds   = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "# Performance optimizations\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds   = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "ykOf1h70TyMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4638253-7925-4db4-fe1d-7fba1a798156"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 838 files belonging to 4 classes.\n",
            "Found 80 files belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Troubleshooting (?)\n",
        "\n",
        "For when the code blows up."
      ],
      "metadata": {
        "id": "i9ZlmaC4bsgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train batches:\", train_ds.cardinality().numpy())\n",
        "print(\"Val batches:\", val_ds.cardinality().numpy())\n",
        "\n",
        "print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/COS30049 - PlantRecog/dataset/Validation\"\n",
        "bad_files = []\n",
        "for root, dirs, files in os.walk(data_dir):\n",
        "    for fname in files:\n",
        "        fpath = os.path.join(root, fname)\n",
        "        try:\n",
        "            img = Image.open(fpath)    # try open\n",
        "            img.verify()               # verify integrity\n",
        "        except Exception as e:\n",
        "            print(f\"Corrupted: {fpath} ({e})\")\n",
        "            bad_files.append(fpath)\n",
        "\n",
        "print(\"Bad files:\", bad_files)\n",
        "\n",
        "import os\n",
        "for root, dirs, files in os.walk(data_dir):\n",
        "    for f in files:\n",
        "        if not f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "            print(\"Non-image file:\", os.path.join(root, f))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5UHaWd4blg4",
        "outputId": "aa3eed82-2adb-474b-b327-4048626650c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 27\n",
            "Val batches: 3\n",
            "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Bad files: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Model Training\n",
        "\n",
        "This step will then commence model training with the base as EfficientNetB0 and weights from ImageNet while not including the head classifier, which is defined ourself.\n"
      ],
      "metadata": {
        "id": "ILKS7vxbXBHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(224,224,3))\n",
        "base.trainable = False\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(base.output)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = models.Model(base.input, out)\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Callbacks (recommended)\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"best_model.h5\", save_best_only=True),\n",
        "    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=callbacks)\n",
        "\n",
        "# Fine-tune: unfreeze last few layers or whole base\n",
        "# Option A: Unfreeze whole base (careful with memory / lr)\n",
        "base.trainable = True\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=callbacks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCtHXJbxXARz",
        "outputId": "9691955c-f659-4f2a-f4eb-d6103720f93f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Progress Saving\n",
        "\n",
        "Once satisfied with the results, hit this to update the whole model. This includes saving the model trained, architecture, weights and optimiser."
      ],
      "metadata": {
        "id": "_05HheRXX_2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save entire model (recommended for checkpointing)\n",
        "model.save(\"/content/drive/MyDrive/plant_identifier/efficientnet_model\")\n"
      ],
      "metadata": {
        "id": "rCnvPi2VX-1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.1 Model Testing\n",
        "\n",
        "Use this block to tweak paths to images for testing or loading the whole model to be used in an application. Ideally should be used to be deployed on a Gradio site for quick testing before integrating into the mobile app."
      ],
      "metadata": {
        "id": "ucKfJwVvYJbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Load model\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/plant_identifier/efficientnet_model\")\n",
        "\n",
        "# Load a test image\n",
        "img_path = \"/content/drive/MyDrive/test_images/mango1.jpg\"\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)  # add batch dimension\n",
        "\n",
        "# Normalize to match training\n",
        "img_array = img_array / 255.0\n",
        "\n",
        "# Predict\n",
        "preds = model.predict(img_array)\n",
        "class_names = train_ds.class_names  # assumes you kept this list saved\n",
        "print(\"Prediction:\", class_names[np.argmax(preds)])\n"
      ],
      "metadata": {
        "id": "JayxiM91YYdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.2 Model Checkpoint Loading\n",
        "\n",
        "Use this block to load model and continue retraining (in progress)."
      ],
      "metadata": {
        "id": "rL8ORkBMYgFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Load model\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/plant_identifier/efficientnet_model\")\n",
        "\n",
        "# Load a test image\n",
        "img_path = \"/content/drive/MyDrive/test_images/mango1.jpg\"\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)  # add batch dimension\n",
        "\n",
        "# Normalize to match training\n",
        "img_array = img_array / 255.0\n",
        "\n",
        "# Predict\n",
        "preds = model.predict(img_array)\n",
        "class_names = train_ds.class_names  # assumes you kept this list saved\n",
        "print(\"Prediction:\", class_names[np.argmax(preds)])\n"
      ],
      "metadata": {
        "id": "cVk93tZLYn26"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}